{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Deep Learning models\n",
    "When possible, it is a good idea to fine-tune existing CNN that have been pre-trained on large dataset. This can be done when the dataset under consideration is similar in context to the dataset that has been used to pre-train the model. We can then used a state of the art CNN that has already learned features that are relevant to our own deep learning task. We are then only left with calculating the bottleneck features of the pre-trained CNN and fine-tuned the pre-trained weights.\n",
    "\n",
    "In the following, we use the ResNet50 deep learning model. The model is available in keras along with its weights pre-trained on the [ImageNet](http://www.image-net.org/) dataset, a collection of 1.2M labeled images of 1000 object categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog categories: 133\n",
      "Dog images: 8351 \n",
      "\n",
      "Training dog images: 6680\n",
      "Validation dog images: 835\n",
      "Test dog images: 836\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, y_train = load_dataset('dogImages/train')\n",
    "valid_files, y_valid = load_dataset('dogImages/valid')\n",
    "test_files, y_test = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[4:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('Dog categories: %d' % len(dog_names))\n",
    "print('Dog images: %d \\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('Training dog images: %d' % len(train_files))\n",
    "print('Validation dog images: %d' % len(valid_files))\n",
    "print('Test dog images: %d' % len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2f66288e7940708a3a8c61532677b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6680), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4046c0bf0f6c43f1952e38d71390d932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=835), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52621b3165f48c99f3f6fc01170cdbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=836), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def path_to_image(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_image(img_paths):\n",
    "    list_of_images = [path_to_image(img_path) for img_path in tqdm_notebook(img_paths)]\n",
    "    return np.vstack(list_of_images)\n",
    "\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "x_train = paths_to_image(train_files).astype('float32')/255\n",
    "x_valid = paths_to_image(valid_files).astype('float32')/255\n",
    "x_test = paths_to_image(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, None, None, 6 9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, None, None, 6 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, None, 6 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 6 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, None, None, 6 4160        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, None, None, 6 256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, None, 6 0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, None, None, 6 36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, None, None, 6 256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, None, 6 0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, None, None, 2 16640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, None, None, 2 16640       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, None, None, 2 1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, None, None, 2 1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, None, 2 0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, None, 2 0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, None, None, 6 16448       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, None, None, 6 256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, None, 6 0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, None, None, 6 36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, None, None, 6 256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, None, None, 6 0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, None, None, 2 16640       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, None, None, 2 1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, None, None, 2 0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, None, 2 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, None, None, 6 16448       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, None, None, 6 256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, None, 6 0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, None, None, 6 36928       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, None, None, 6 256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, None, 6 0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, None, None, 2 16640       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, None, None, 2 1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, None, None, 2 0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, None, 2 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, None, None, 1 32896       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, None, None, 1 512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None, None, 1 0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, None, None, 1 147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, None, None, 1 512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None, None, 1 0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, None, None, 5 66048       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, None, None, 5 131584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, None, None, 5 2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, None, None, 5 2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, None, None, 5 0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, None, None, 5 0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, None, None, 1 65664       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, None, None, 1 512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, None, 1 0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, None, None, 1 147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, None, None, 1 512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, None, 1 0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, None, None, 5 66048       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, None, None, 5 2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, None, None, 5 0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, None, 5 0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, None, None, 1 65664       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, None, None, 1 512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, None, None, 1 0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, None, None, 1 147584      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, None, None, 1 512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, None, None, 1 0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, None, None, 5 66048       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, None, None, 5 2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, None, None, 5 0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, None, None, 5 0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, None, None, 1 65664       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, None, None, 1 512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, None, None, 1 0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, None, None, 1 147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, None, None, 1 512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, None, None, 1 0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, None, None, 5 66048       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, None, None, 5 2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, None, None, 5 0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, None, None, 5 0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, None, None, 2 131328      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, None, None, 2 1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, None, None, 2 0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, None, None, 2 590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, None, None, 2 1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, None, None, 2 0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, None, None, 1 263168      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, None, None, 1 525312      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, None, None, 1 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, None, None, 1 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, None, None, 1 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, None, None, 1 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, None, None, 2 262400      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, None, None, 2 1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, None, None, 2 0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, None, None, 2 590080      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, None, None, 2 1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, None, None, 2 0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, None, None, 1 263168      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, None, None, 1 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, None, None, 1 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, None, None, 1 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, None, None, 2 262400      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, None, None, 2 1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, None, None, 2 0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, None, None, 2 590080      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, None, None, 2 1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, None, None, 2 0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, None, None, 1 263168      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, None, None, 1 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, None, None, 1 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, None, None, 1 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, None, None, 2 262400      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, None, None, 2 1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, None, None, 2 0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, None, None, 2 590080      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, None, None, 2 1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, None, None, 2 0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, None, None, 1 263168      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, None, None, 1 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, None, None, 1 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, None, None, 1 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, None, None, 2 262400      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, None, None, 2 1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, None, None, 2 0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, None, None, 2 590080      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, None, None, 2 1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, None, None, 2 0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, None, None, 1 263168      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, None, None, 1 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, None, None, 1 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, None, None, 1 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, None, None, 2 262400      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, None, None, 2 1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, None, None, 2 0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, None, None, 2 590080      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, None, None, 2 1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, None, None, 2 0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, None, None, 1 263168      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, None, None, 1 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, None, None, 1 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, None, None, 1 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, None, None, 5 524800      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, None, None, 5 2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, None, None, 5 0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, None, None, 5 2359808     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, None, None, 5 2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, None, None, 5 0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, None, None, 2 1050624     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, None, None, 2 2099200     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, None, None, 2 8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, None, None, 2 8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, None, None, 2 0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, None, None, 2 0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, None, None, 5 1049088     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, None, None, 5 2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, None, None, 5 0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, None, None, 5 2359808     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, None, None, 5 2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, None, None, 5 0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, None, None, 2 1050624     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, None, None, 2 8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, None, None, 2 0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, None, None, 2 0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, None, None, 5 1049088     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, None, None, 5 2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, None, None, 5 0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, None, None, 5 2359808     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, None, None, 5 2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, None, None, 5 0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, None, None, 2 1050624     activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, None, None, 2 8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, None, None, 2 0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, None, None, 2 0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, None, None, 2 0           activation_49[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "ResNet50_truncated = ResNet50(weights='imagenet', include_top=False)\n",
    "ResNet50_truncated.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "\n",
    "addon = ResNet50_truncated.output\n",
    "# add a drop out layer\n",
    "addon = Dropout(0.3)(addon)\n",
    "# add a global spatial average pooling layer\n",
    "addon = GlobalAveragePooling2D()(addon)\n",
    "# let's add a fully-connected layer\n",
    "addon = Dense(512, activation='relu')(addon)\n",
    "# and a logistic layer -- 133 classes\n",
    "predictions = Dense(133, activation='softmax')(addon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6680/6680 [==============================] - 70s 10ms/step - loss: 2.8685 - acc: 0.3207 - val_loss: 6.8232 - val_acc: 0.0096\n",
      "Epoch 2/15\n",
      "6680/6680 [==============================] - 69s 10ms/step - loss: 1.2610 - acc: 0.6268 - val_loss: 8.1386 - val_acc: 0.0120\n",
      "Epoch 3/15\n",
      "6680/6680 [==============================] - 69s 10ms/step - loss: 0.8999 - acc: 0.7210 - val_loss: 6.8393 - val_acc: 0.0623\n",
      "Epoch 4/15\n",
      "6680/6680 [==============================] - 69s 10ms/step - loss: 0.6771 - acc: 0.7844 - val_loss: 3.8715 - val_acc: 0.3030\n",
      "Epoch 5/15\n",
      "6680/6680 [==============================] - 69s 10ms/step - loss: 0.5497 - acc: 0.8237 - val_loss: 2.0993 - val_acc: 0.5473\n",
      "Epoch 6/15\n",
      "6680/6680 [==============================] - 69s 10ms/step - loss: 0.4568 - acc: 0.8481 - val_loss: 1.4454 - val_acc: 0.6479\n",
      "Epoch 7/15\n",
      "6680/6680 [==============================] - 69s 10ms/step - loss: 0.4025 - acc: 0.8696 - val_loss: 1.5882 - val_acc: 0.6551\n",
      "Epoch 8/15\n",
      "6680/6680 [==============================] - 69s 10ms/step - loss: 0.3395 - acc: 0.8873 - val_loss: 1.4683 - val_acc: 0.6575\n",
      "Epoch 9/15\n",
      "6680/6680 [==============================] - 69s 10ms/step - loss: 0.3052 - acc: 0.9022 - val_loss: 1.5422 - val_acc: 0.6599\n",
      "Epoch 10/15\n",
      "6680/6680 [==============================] - 69s 10ms/step - loss: 0.2803 - acc: 0.9078 - val_loss: 1.5033 - val_acc: 0.6599\n",
      "Epoch 11/15\n",
      "6680/6680 [==============================] - 70s 10ms/step - loss: 0.2666 - acc: 0.9127 - val_loss: 1.5005 - val_acc: 0.6575\n",
      "Epoch 12/15\n",
      "6680/6680 [==============================] - 70s 10ms/step - loss: 0.2454 - acc: 0.9189 - val_loss: 1.7294 - val_acc: 0.6635\n",
      "Epoch 13/15\n",
      "6680/6680 [==============================] - 70s 10ms/step - loss: 0.2368 - acc: 0.9238 - val_loss: 1.7492 - val_acc: 0.6563\n",
      "Epoch 14/15\n",
      "6680/6680 [==============================] - 70s 10ms/step - loss: 0.2113 - acc: 0.9314 - val_loss: 1.8317 - val_acc: 0.6659\n",
      "Epoch 15/15\n",
      "6680/6680 [==============================] - 70s 10ms/step - loss: 0.1955 - acc: 0.9362 - val_loss: 2.0877 - val_acc: 0.6431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f213e1937f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model = Model(inputs=ResNet50_truncated.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional ResNet50 layers\n",
    "for layer in ResNet50_truncated.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fit\n",
    "model.fit(x_train, y_train, validation_data = (x_valid, y_valid), \n",
    "          epochs = 15, batch_size = 32, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 conv1\n",
      "2 bn_conv1\n",
      "3 activation_1\n",
      "4 max_pooling2d_1\n",
      "5 res2a_branch2a\n",
      "6 bn2a_branch2a\n",
      "7 activation_2\n",
      "8 res2a_branch2b\n",
      "9 bn2a_branch2b\n",
      "10 activation_3\n",
      "11 res2a_branch2c\n",
      "12 res2a_branch1\n",
      "13 bn2a_branch2c\n",
      "14 bn2a_branch1\n",
      "15 add_1\n",
      "16 activation_4\n",
      "17 res2b_branch2a\n",
      "18 bn2b_branch2a\n",
      "19 activation_5\n",
      "20 res2b_branch2b\n",
      "21 bn2b_branch2b\n",
      "22 activation_6\n",
      "23 res2b_branch2c\n",
      "24 bn2b_branch2c\n",
      "25 add_2\n",
      "26 activation_7\n",
      "27 res2c_branch2a\n",
      "28 bn2c_branch2a\n",
      "29 activation_8\n",
      "30 res2c_branch2b\n",
      "31 bn2c_branch2b\n",
      "32 activation_9\n",
      "33 res2c_branch2c\n",
      "34 bn2c_branch2c\n",
      "35 add_3\n",
      "36 activation_10\n",
      "37 res3a_branch2a\n",
      "38 bn3a_branch2a\n",
      "39 activation_11\n",
      "40 res3a_branch2b\n",
      "41 bn3a_branch2b\n",
      "42 activation_12\n",
      "43 res3a_branch2c\n",
      "44 res3a_branch1\n",
      "45 bn3a_branch2c\n",
      "46 bn3a_branch1\n",
      "47 add_4\n",
      "48 activation_13\n",
      "49 res3b_branch2a\n",
      "50 bn3b_branch2a\n",
      "51 activation_14\n",
      "52 res3b_branch2b\n",
      "53 bn3b_branch2b\n",
      "54 activation_15\n",
      "55 res3b_branch2c\n",
      "56 bn3b_branch2c\n",
      "57 add_5\n",
      "58 activation_16\n",
      "59 res3c_branch2a\n",
      "60 bn3c_branch2a\n",
      "61 activation_17\n",
      "62 res3c_branch2b\n",
      "63 bn3c_branch2b\n",
      "64 activation_18\n",
      "65 res3c_branch2c\n",
      "66 bn3c_branch2c\n",
      "67 add_6\n",
      "68 activation_19\n",
      "69 res3d_branch2a\n",
      "70 bn3d_branch2a\n",
      "71 activation_20\n",
      "72 res3d_branch2b\n",
      "73 bn3d_branch2b\n",
      "74 activation_21\n",
      "75 res3d_branch2c\n",
      "76 bn3d_branch2c\n",
      "77 add_7\n",
      "78 activation_22\n",
      "79 res4a_branch2a\n",
      "80 bn4a_branch2a\n",
      "81 activation_23\n",
      "82 res4a_branch2b\n",
      "83 bn4a_branch2b\n",
      "84 activation_24\n",
      "85 res4a_branch2c\n",
      "86 res4a_branch1\n",
      "87 bn4a_branch2c\n",
      "88 bn4a_branch1\n",
      "89 add_8\n",
      "90 activation_25\n",
      "91 res4b_branch2a\n",
      "92 bn4b_branch2a\n",
      "93 activation_26\n",
      "94 res4b_branch2b\n",
      "95 bn4b_branch2b\n",
      "96 activation_27\n",
      "97 res4b_branch2c\n",
      "98 bn4b_branch2c\n",
      "99 add_9\n",
      "100 activation_28\n",
      "101 res4c_branch2a\n",
      "102 bn4c_branch2a\n",
      "103 activation_29\n",
      "104 res4c_branch2b\n",
      "105 bn4c_branch2b\n",
      "106 activation_30\n",
      "107 res4c_branch2c\n",
      "108 bn4c_branch2c\n",
      "109 add_10\n",
      "110 activation_31\n",
      "111 res4d_branch2a\n",
      "112 bn4d_branch2a\n",
      "113 activation_32\n",
      "114 res4d_branch2b\n",
      "115 bn4d_branch2b\n",
      "116 activation_33\n",
      "117 res4d_branch2c\n",
      "118 bn4d_branch2c\n",
      "119 add_11\n",
      "120 activation_34\n",
      "121 res4e_branch2a\n",
      "122 bn4e_branch2a\n",
      "123 activation_35\n",
      "124 res4e_branch2b\n",
      "125 bn4e_branch2b\n",
      "126 activation_36\n",
      "127 res4e_branch2c\n",
      "128 bn4e_branch2c\n",
      "129 add_12\n",
      "130 activation_37\n",
      "131 res4f_branch2a\n",
      "132 bn4f_branch2a\n",
      "133 activation_38\n",
      "134 res4f_branch2b\n",
      "135 bn4f_branch2b\n",
      "136 activation_39\n",
      "137 res4f_branch2c\n",
      "138 bn4f_branch2c\n",
      "139 add_13\n",
      "140 activation_40\n",
      "141 res5a_branch2a\n",
      "142 bn5a_branch2a\n",
      "143 activation_41\n",
      "144 res5a_branch2b\n",
      "145 bn5a_branch2b\n",
      "146 activation_42\n",
      "147 res5a_branch2c\n",
      "148 res5a_branch1\n",
      "149 bn5a_branch2c\n",
      "150 bn5a_branch1\n",
      "151 add_14\n",
      "152 activation_43\n",
      "153 res5b_branch2a\n",
      "154 bn5b_branch2a\n",
      "155 activation_44\n",
      "156 res5b_branch2b\n",
      "157 bn5b_branch2b\n",
      "158 activation_45\n",
      "159 res5b_branch2c\n",
      "160 bn5b_branch2c\n",
      "161 add_15\n",
      "162 activation_46\n",
      "163 res5c_branch2a\n",
      "164 bn5c_branch2a\n",
      "165 activation_47\n",
      "166 res5c_branch2b\n",
      "167 bn5c_branch2b\n",
      "168 activation_48\n",
      "169 res5c_branch2c\n",
      "170 bn5c_branch2c\n",
      "171 add_16\n",
      "172 activation_49\n",
      "173 avg_pool\n"
     ]
    }
   ],
   "source": [
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(ResNet50_truncated.layers):\n",
    "   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[161:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "opt = SGD(lr=0.0001, momentum=0.9, decay=1e-6, nesterov=True)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9504Epoch 00001: val_loss improved from inf to 1.76799, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 76s 11ms/step - loss: 0.1609 - acc: 0.9506 - val_loss: 1.7680 - val_acc: 0.6695\n",
      "Epoch 2/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9674Epoch 00002: val_loss improved from 1.76799 to 1.66617, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 73s 11ms/step - loss: 0.1026 - acc: 0.9675 - val_loss: 1.6662 - val_acc: 0.6766\n",
      "Epoch 3/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9736Epoch 00003: val_loss improved from 1.66617 to 1.60960, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0771 - acc: 0.9735 - val_loss: 1.6096 - val_acc: 0.6862\n",
      "Epoch 4/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9740Epoch 00004: val_loss improved from 1.60960 to 1.57069, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0782 - acc: 0.9738 - val_loss: 1.5707 - val_acc: 0.6970\n",
      "Epoch 5/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9805Epoch 00005: val_loss improved from 1.57069 to 1.55966, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0598 - acc: 0.9804 - val_loss: 1.5597 - val_acc: 0.6982\n",
      "Epoch 6/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9788Epoch 00006: val_loss improved from 1.55966 to 1.53981, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 75s 11ms/step - loss: 0.0581 - acc: 0.9786 - val_loss: 1.5398 - val_acc: 0.7030\n",
      "Epoch 7/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9806Epoch 00007: val_loss improved from 1.53981 to 1.53553, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 75s 11ms/step - loss: 0.0551 - acc: 0.9807 - val_loss: 1.5355 - val_acc: 0.6994\n",
      "Epoch 8/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9803Epoch 00008: val_loss improved from 1.53553 to 1.51915, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 75s 11ms/step - loss: 0.0556 - acc: 0.9804 - val_loss: 1.5191 - val_acc: 0.7042\n",
      "Epoch 9/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9848Epoch 00009: val_loss improved from 1.51915 to 1.50450, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0462 - acc: 0.9844 - val_loss: 1.5045 - val_acc: 0.7030\n",
      "Epoch 10/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9835Epoch 00010: val_loss improved from 1.50450 to 1.49986, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 75s 11ms/step - loss: 0.0494 - acc: 0.9834 - val_loss: 1.4999 - val_acc: 0.7030\n",
      "Epoch 11/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9827Epoch 00011: val_loss improved from 1.49986 to 1.49165, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 75s 11ms/step - loss: 0.0524 - acc: 0.9826 - val_loss: 1.4916 - val_acc: 0.7006\n",
      "Epoch 12/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9863Epoch 00012: val_loss improved from 1.49165 to 1.48784, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0406 - acc: 0.9864 - val_loss: 1.4878 - val_acc: 0.6994\n",
      "Epoch 13/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9850Epoch 00013: val_loss improved from 1.48784 to 1.48290, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 75s 11ms/step - loss: 0.0438 - acc: 0.9849 - val_loss: 1.4829 - val_acc: 0.6982\n",
      "Epoch 14/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9860Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0417 - acc: 0.9861 - val_loss: 1.4865 - val_acc: 0.7018\n",
      "Epoch 15/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9848Epoch 00015: val_loss improved from 1.48290 to 1.48192, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0477 - acc: 0.9849 - val_loss: 1.4819 - val_acc: 0.6994\n",
      "Epoch 16/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9889Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0357 - acc: 0.9889 - val_loss: 1.4845 - val_acc: 0.7042\n",
      "Epoch 17/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9895Epoch 00017: val_loss improved from 1.48192 to 1.47991, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0349 - acc: 0.9894 - val_loss: 1.4799 - val_acc: 0.7042\n",
      "Epoch 18/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9877Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0388 - acc: 0.9877 - val_loss: 1.4821 - val_acc: 0.7078\n",
      "Epoch 19/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9887Epoch 00019: val_loss improved from 1.47991 to 1.47476, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0333 - acc: 0.9888 - val_loss: 1.4748 - val_acc: 0.7102\n",
      "Epoch 20/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9899Epoch 00020: val_loss improved from 1.47476 to 1.46667, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0350 - acc: 0.9900 - val_loss: 1.4667 - val_acc: 0.7102\n",
      "Epoch 21/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9887Epoch 00021: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0348 - acc: 0.9888 - val_loss: 1.4669 - val_acc: 0.7174\n",
      "Epoch 22/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9881Epoch 00022: val_loss improved from 1.46667 to 1.46365, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 75s 11ms/step - loss: 0.0344 - acc: 0.9882 - val_loss: 1.4636 - val_acc: 0.7102\n",
      "Epoch 23/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9896Epoch 00023: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0353 - acc: 0.9897 - val_loss: 1.4681 - val_acc: 0.7078\n",
      "Epoch 24/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9899Epoch 00024: val_loss improved from 1.46365 to 1.46193, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 75s 11ms/step - loss: 0.0303 - acc: 0.9898 - val_loss: 1.4619 - val_acc: 0.7054\n",
      "Epoch 25/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9899Epoch 00025: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0344 - acc: 0.9898 - val_loss: 1.4645 - val_acc: 0.7066\n",
      "Epoch 26/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9901Epoch 00026: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0287 - acc: 0.9901 - val_loss: 1.4629 - val_acc: 0.7090\n",
      "Epoch 27/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9910Epoch 00027: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0270 - acc: 0.9910 - val_loss: 1.4626 - val_acc: 0.7102\n",
      "Epoch 28/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9907Epoch 00028: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0328 - acc: 0.9907 - val_loss: 1.4671 - val_acc: 0.7090\n",
      "Epoch 29/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9896Epoch 00029: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0320 - acc: 0.9897 - val_loss: 1.4696 - val_acc: 0.7054\n",
      "Epoch 30/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9905Epoch 00030: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0331 - acc: 0.9906 - val_loss: 1.4685 - val_acc: 0.7114\n",
      "Epoch 31/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9916Epoch 00031: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0276 - acc: 0.9916 - val_loss: 1.4716 - val_acc: 0.7054\n",
      "Epoch 32/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9899Epoch 00032: val_loss improved from 1.46193 to 1.46182, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0334 - acc: 0.9900 - val_loss: 1.4618 - val_acc: 0.7042\n",
      "Epoch 33/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9911Epoch 00033: val_loss improved from 1.46182 to 1.46054, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0270 - acc: 0.9912 - val_loss: 1.4605 - val_acc: 0.7102\n",
      "Epoch 34/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9919Epoch 00034: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0277 - acc: 0.9919 - val_loss: 1.4617 - val_acc: 0.7102\n",
      "Epoch 35/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9932Epoch 00035: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0251 - acc: 0.9933 - val_loss: 1.4648 - val_acc: 0.7114\n",
      "Epoch 36/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9923Epoch 00036: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0267 - acc: 0.9924 - val_loss: 1.4618 - val_acc: 0.7102\n",
      "Epoch 37/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9910Epoch 00037: val_loss improved from 1.46054 to 1.45278, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0330 - acc: 0.9910 - val_loss: 1.4528 - val_acc: 0.7114\n",
      "Epoch 38/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9920Epoch 00038: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0278 - acc: 0.9921 - val_loss: 1.4567 - val_acc: 0.7102\n",
      "Epoch 39/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9932Epoch 00039: val_loss improved from 1.45278 to 1.45000, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0249 - acc: 0.9933 - val_loss: 1.4500 - val_acc: 0.7066\n",
      "Epoch 40/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9917Epoch 00040: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0272 - acc: 0.9918 - val_loss: 1.4504 - val_acc: 0.7090\n",
      "Epoch 41/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9911Epoch 00041: val_loss improved from 1.45000 to 1.44606, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0287 - acc: 0.9912 - val_loss: 1.4461 - val_acc: 0.7054\n",
      "Epoch 42/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9914Epoch 00042: val_loss improved from 1.44606 to 1.44491, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 75s 11ms/step - loss: 0.0279 - acc: 0.9915 - val_loss: 1.4449 - val_acc: 0.7090\n",
      "Epoch 43/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9893Epoch 00043: val_loss improved from 1.44491 to 1.44419, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 75s 11ms/step - loss: 0.0311 - acc: 0.9892 - val_loss: 1.4442 - val_acc: 0.7102\n",
      "Epoch 44/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9922Epoch 00044: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0253 - acc: 0.9922 - val_loss: 1.4443 - val_acc: 0.7090\n",
      "Epoch 45/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9919Epoch 00045: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0271 - acc: 0.9918 - val_loss: 1.4455 - val_acc: 0.7090\n",
      "Epoch 46/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9926Epoch 00046: val_loss improved from 1.44419 to 1.44016, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 75s 11ms/step - loss: 0.0268 - acc: 0.9927 - val_loss: 1.4402 - val_acc: 0.7102\n",
      "Epoch 47/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9920Epoch 00047: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0238 - acc: 0.9921 - val_loss: 1.4416 - val_acc: 0.7114\n",
      "Epoch 48/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9935Epoch 00048: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0237 - acc: 0.9936 - val_loss: 1.4444 - val_acc: 0.7102\n",
      "Epoch 49/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9925Epoch 00049: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0268 - acc: 0.9925 - val_loss: 1.4458 - val_acc: 0.7102\n",
      "Epoch 50/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9917Epoch 00050: val_loss improved from 1.44016 to 1.43775, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0246 - acc: 0.9916 - val_loss: 1.4378 - val_acc: 0.7114\n",
      "Epoch 51/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9925Epoch 00051: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0259 - acc: 0.9925 - val_loss: 1.4408 - val_acc: 0.7126\n",
      "Epoch 52/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9931Epoch 00052: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0250 - acc: 0.9931 - val_loss: 1.4446 - val_acc: 0.7102\n",
      "Epoch 53/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9926Epoch 00053: val_loss improved from 1.43775 to 1.43685, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0237 - acc: 0.9927 - val_loss: 1.4369 - val_acc: 0.7102\n",
      "Epoch 54/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9934Epoch 00054: val_loss improved from 1.43685 to 1.43520, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0226 - acc: 0.9933 - val_loss: 1.4352 - val_acc: 0.7150\n",
      "Epoch 55/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9934Epoch 00055: val_loss improved from 1.43520 to 1.42954, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 75s 11ms/step - loss: 0.0231 - acc: 0.9934 - val_loss: 1.4295 - val_acc: 0.7138\n",
      "Epoch 56/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9931Epoch 00056: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0249 - acc: 0.9931 - val_loss: 1.4313 - val_acc: 0.7126\n",
      "Epoch 57/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9928Epoch 00057: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0267 - acc: 0.9928 - val_loss: 1.4341 - val_acc: 0.7102\n",
      "Epoch 58/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9925Epoch 00058: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0245 - acc: 0.9925 - val_loss: 1.4344 - val_acc: 0.7090\n",
      "Epoch 59/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9947Epoch 00059: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0197 - acc: 0.9948 - val_loss: 1.4314 - val_acc: 0.7078\n",
      "Epoch 60/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9925Epoch 00060: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0243 - acc: 0.9925 - val_loss: 1.4345 - val_acc: 0.7090\n",
      "Epoch 61/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9946Epoch 00061: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0200 - acc: 0.9946 - val_loss: 1.4353 - val_acc: 0.7066\n",
      "Epoch 62/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9937Epoch 00062: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0214 - acc: 0.9937 - val_loss: 1.4323 - val_acc: 0.7066\n",
      "Epoch 63/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9943Epoch 00063: val_loss improved from 1.42954 to 1.42893, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0203 - acc: 0.9942 - val_loss: 1.4289 - val_acc: 0.7066\n",
      "Epoch 64/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9928Epoch 00064: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0252 - acc: 0.9927 - val_loss: 1.4342 - val_acc: 0.7066\n",
      "Epoch 65/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9917Epoch 00065: val_loss improved from 1.42893 to 1.42690, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0267 - acc: 0.9918 - val_loss: 1.4269 - val_acc: 0.7126\n",
      "Epoch 66/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9931Epoch 00066: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0222 - acc: 0.9931 - val_loss: 1.4326 - val_acc: 0.7114\n",
      "Epoch 67/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9950Epoch 00067: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0188 - acc: 0.9951 - val_loss: 1.4341 - val_acc: 0.7114\n",
      "Epoch 68/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9928Epoch 00068: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0212 - acc: 0.9927 - val_loss: 1.4330 - val_acc: 0.7102\n",
      "Epoch 69/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9922Epoch 00069: val_loss improved from 1.42690 to 1.42509, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0237 - acc: 0.9922 - val_loss: 1.4251 - val_acc: 0.7102\n",
      "Epoch 70/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9946Epoch 00070: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0187 - acc: 0.9945 - val_loss: 1.4362 - val_acc: 0.7090\n",
      "Epoch 71/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9929Epoch 00071: val_loss improved from 1.42509 to 1.42226, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0220 - acc: 0.9930 - val_loss: 1.4223 - val_acc: 0.7090\n",
      "Epoch 72/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9938Epoch 00072: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0206 - acc: 0.9939 - val_loss: 1.4233 - val_acc: 0.7078\n",
      "Epoch 73/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9928Epoch 00073: val_loss improved from 1.42226 to 1.42144, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 75s 11ms/step - loss: 0.0217 - acc: 0.9928 - val_loss: 1.4214 - val_acc: 0.7114\n",
      "Epoch 74/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9949Epoch 00074: val_loss improved from 1.42144 to 1.41710, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 75s 11ms/step - loss: 0.0178 - acc: 0.9949 - val_loss: 1.4171 - val_acc: 0.7126\n",
      "Epoch 75/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9944Epoch 00075: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0240 - acc: 0.9943 - val_loss: 1.4243 - val_acc: 0.7102\n",
      "Epoch 76/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9943Epoch 00076: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0196 - acc: 0.9943 - val_loss: 1.4224 - val_acc: 0.7102\n",
      "Epoch 77/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9932Epoch 00077: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0227 - acc: 0.9931 - val_loss: 1.4188 - val_acc: 0.7174\n",
      "Epoch 78/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9940Epoch 00078: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0227 - acc: 0.9940 - val_loss: 1.4221 - val_acc: 0.7138\n",
      "Epoch 79/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9940Epoch 00079: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0207 - acc: 0.9940 - val_loss: 1.4205 - val_acc: 0.7126\n",
      "Epoch 80/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9937Epoch 00080: val_loss improved from 1.41710 to 1.41700, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 75s 11ms/step - loss: 0.0194 - acc: 0.9937 - val_loss: 1.4170 - val_acc: 0.7138\n",
      "Epoch 81/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9943Epoch 00081: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0199 - acc: 0.9943 - val_loss: 1.4244 - val_acc: 0.7138\n",
      "Epoch 82/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9938Epoch 00082: val_loss improved from 1.41700 to 1.41476, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 75s 11ms/step - loss: 0.0197 - acc: 0.9939 - val_loss: 1.4148 - val_acc: 0.7162\n",
      "Epoch 83/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9952Epoch 00083: val_loss improved from 1.41476 to 1.40980, saving model to dog.best.hdf5\n",
      "6680/6680 [==============================] - 75s 11ms/step - loss: 0.0177 - acc: 0.9952 - val_loss: 1.4098 - val_acc: 0.7162\n",
      "Epoch 84/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9932Epoch 00084: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0206 - acc: 0.9933 - val_loss: 1.4174 - val_acc: 0.7126\n",
      "Epoch 85/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9938Epoch 00085: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0214 - acc: 0.9939 - val_loss: 1.4196 - val_acc: 0.7186\n",
      "Epoch 86/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9947Epoch 00086: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0178 - acc: 0.9948 - val_loss: 1.4222 - val_acc: 0.7138\n",
      "Epoch 87/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9950Epoch 00087: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0176 - acc: 0.9951 - val_loss: 1.4177 - val_acc: 0.7174\n",
      "Epoch 88/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9949Epoch 00088: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0195 - acc: 0.9949 - val_loss: 1.4212 - val_acc: 0.7174\n",
      "Epoch 89/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9938Epoch 00089: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0201 - acc: 0.9937 - val_loss: 1.4184 - val_acc: 0.7162\n",
      "Epoch 90/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9937Epoch 00090: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0209 - acc: 0.9937 - val_loss: 1.4225 - val_acc: 0.7150\n",
      "Epoch 91/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9955Epoch 00091: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0170 - acc: 0.9955 - val_loss: 1.4194 - val_acc: 0.7138\n",
      "Epoch 92/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9970Epoch 00092: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0141 - acc: 0.9970 - val_loss: 1.4191 - val_acc: 0.7138\n",
      "Epoch 93/100\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9958Epoch 00093: val_loss did not improve\n",
      "6680/6680 [==============================] - 74s 11ms/step - loss: 0.0152 - acc: 0.9958 - val_loss: 1.4178 - val_acc: 0.7138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2123ad4f28>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, History\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='dog.best.hdf5', verbose=1, save_best_only=True)\n",
    "checkimprovement = EarlyStopping(monitor='val_loss', min_delta=0, patience=10)\n",
    "fit = History()\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=100, batch_size=32, \n",
    "          callbacks=[checkpointer,checkimprovement,fit], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4IAAAFACAYAAADptsL3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICgiIiIiIpJiFAiKiIiIiIikmP8HAPbPDqRVpMUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f21226ac908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def history(fit):\n",
    "    plt.figure(figsize = (15, 5) )\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.plot(fit.history['loss'], color = 'blue', label = 'train')\n",
    "    plt.plot(fit.history['val_loss'], color = 'red', label = 'valid')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss Function')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(fit.history['acc'], color = 'blue', label = 'train')\n",
    "    plt.plot(fit.history['val_acc'], color = 'red', label = 'valid')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "history(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 73.56%\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model.load_weights('dog.best.hdf5')\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = [np.argmax(y_hat) for y_hat in model.predict(x_test)]\n",
    "y_true = [np.argmax(y) for y in y_test]\n",
    "print('Test accuracy: %.2f%%' % (accuracy_score(y_true, y_pred)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
